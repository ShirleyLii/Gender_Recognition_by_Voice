{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np  \n",
    "import random\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%store -r train_x\n",
    "%store -r test_x\n",
    "%store -r train_y\n",
    "%store -r test_y\n",
    "\n",
    "%store -r train_x_two_features\n",
    "%store -r test_x_two_features\n",
    "%store -r train_y_two_features\n",
    "%store -r test_y_two_features\n",
    "\n",
    "%store -r pca_train_x\n",
    "%store -r pca_test_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First approach:\n",
    "We use KNeighborsClassifier to implement the k-nearest neighbors vote.\n",
    "\n",
    "To find the optimal value of k, we will use a simple brute-force strategy. Namely, we will try many values of k and evaluate each, settling for the one with the best prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_k_nearest_neighbour(train_x, test_x, train_y, test_y):\n",
    "    error_rate = []\n",
    "    kvals = range(1,21)  # range of k parameters to test\n",
    "    for i in kvals:\n",
    "        knn = KNeighborsClassifier(n_neighbors=i)\n",
    "        knn.fit(train_x,train_y)\n",
    "        pred_y_i = knn.predict(test_x)\n",
    "        error_rate.append(np.mean(pred_y_i != test_y))\n",
    "    \n",
    "    plt.plot(kvals, error_rate, color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "    plt.title('Error Rate vs. K Value')\n",
    "    plt.xlabel('K')\n",
    "    plt.ylabel('Error Rate')\n",
    "    plt.show()\n",
    "    kloc = error_rate.index(min(error_rate))\n",
    "    print('Lowest error is %s occurs at k=%s.' % (error_rate[kloc], kvals[kloc]))\n",
    "    clf = KNeighborsClassifier(kvals[kloc], 'uniform')\n",
    "    clf.fit(train_x, train_y)\n",
    "    \n",
    "    scores = cross_val_score(clf, train_x, train_y, cv=5)\n",
    "    print(\"In-sample accuracy for KNN: %.10f\" % scores.mean())\n",
    "    scores = cross_val_score(clf, test_x, test_y, cv=5) \n",
    "    print(\"Out-of-sample accuracy for KNN: %.10f\" % scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Approach:\n",
    "We wrote our own method to implement the k-Nearest Neighbors algorithm from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Approach 2:\n",
    "# We wrote our own method to implement the k-Nearest Neighbors algorithm from scratch.\n",
    "\n",
    "import math\n",
    "import operator\n",
    "from scipy.stats import mode\n",
    "\n",
    "\n",
    "# Euclidean distance calculation\n",
    "# In order to make predictions we calculate the similarity between any two given data instances. \n",
    "# This is needed so that we can locate the k most similar data instances in the training dataset for a given member of the test dataset and in turn make a prediction.\n",
    "def calculateDistance(instance1, instance2,length):\n",
    "    distance = 0\n",
    "    for x in range(length):\n",
    "        distance += pow((instance1[x] - instance2[x]),2)\n",
    "    return math.sqrt(distance)\n",
    "\n",
    "\n",
    "# Below is the getNeighbors function that returns k most similar neighbors \n",
    "# from the training set for a given test instance (using the already defined euclideanDistance function)\n",
    "def getNeighbors(train_data, train_labels, test_data_single, k):\n",
    "    distances = []\n",
    "    length = train_data.shape[1]\n",
    "    for x in range(len(train_data)):\n",
    "        dist = calculateDistance(test_data_single, train_data[x],length)\n",
    "        distances.append((train_data[x],train_labels[x],dist))\n",
    "    distances.sort(key=operator.itemgetter(2))  #sort based on 3rd item which is distance\n",
    "    neighbors = []\n",
    "    for x in range(k):\n",
    "        neighbors.append(distances[x][1])  #return top k nearest data's label\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "# The funuction below is used for getting the majority voted response from a number of neighbors.\n",
    "def getResponse(neighbors):\n",
    "    classVotes = {}\n",
    "    for x in range(len(neighbors)):\n",
    "        response = neighbors[x]\n",
    "        if response in classVotes:\n",
    "            classVotes[response] += 1\n",
    "        else:\n",
    "            classVotes[response] = 1\n",
    "    sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedVotes[0][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_my_k_nearest_neighbour(train_x, test_x, train_y, test_y):\n",
    "    accuracy = []\n",
    "    print(\"\")\n",
    "    print(\"Use self-written KNeighbors Classifier:\")\n",
    "    for K in range(1,20):\n",
    "        results = []\n",
    "        for i in range(len(test_x)):\n",
    "            test_single = test_x[i]\n",
    "            neighbor = getNeighbors(train_x, train_y, test_single, K)\n",
    "            res = getResponse(neighbor)\n",
    "            results.append(res)\n",
    "        # We evaluate the accuracy of the model by calculating a ratio of the total correct predictions out of all predictions made (the classification accuracy)\n",
    "        correct = 0\n",
    "        wrong = 0\n",
    "        for j in range(len(test_y)):\n",
    "            if(results[j] == test_y[j]):\n",
    "                correct += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "        accuracy.append(float(correct)/(correct+wrong))\n",
    "        print('Correct rate is %s when K = %s.'  % (float(correct)/(correct+wrong),K))\n",
    "    print('Highest correct rate %s occurs at K = %s.' % (max(accuracy), accuracy.index(max(accuracy))+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Use self-written KNeighbors Classifier:\n",
      "Correct rate is 0.9722222222222222 when K = 1.\n",
      "Correct rate is 0.9722222222222222 when K = 2.\n",
      "Correct rate is 0.9772727272727273 when K = 3.\n",
      "Correct rate is 0.9810606060606061 when K = 4.\n",
      "Correct rate is 0.9810606060606061 when K = 5.\n",
      "Correct rate is 0.9823232323232324 when K = 6.\n",
      "Correct rate is 0.9823232323232324 when K = 7.\n",
      "Correct rate is 0.9823232323232324 when K = 8.\n",
      "Correct rate is 0.9785353535353535 when K = 9.\n",
      "Correct rate is 0.9823232323232324 when K = 10.\n",
      "Correct rate is 0.9772727272727273 when K = 11.\n",
      "Correct rate is 0.9797979797979798 when K = 12.\n",
      "Correct rate is 0.9734848484848485 when K = 13.\n",
      "Correct rate is 0.976010101010101 when K = 14.\n",
      "Correct rate is 0.9709595959595959 when K = 15.\n",
      "Correct rate is 0.9709595959595959 when K = 16.\n",
      "Correct rate is 0.9684343434343434 when K = 17.\n",
      "Correct rate is 0.9696969696969697 when K = 18.\n",
      "Correct rate is 0.9696969696969697 when K = 19.\n",
      "Highest correct rate 0.9823232323232324 occurs at K = 6.\n"
     ]
    }
   ],
   "source": [
    "run_my_k_nearest_neighbour(train_x, test_x, train_y, test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
