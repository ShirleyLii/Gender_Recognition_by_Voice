{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np  \n",
    "import random\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First approach:\n",
    "We use KNeighborsClassifier to implement the k-nearest neighbors vote.\n",
    "\n",
    "To find the optimal value of k, we will use a simple brute-force strategy. Namely, we will try many values of k and evaluate each, settling for the one with the best prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_k_nearest_neighbour(train_x, test_x, train_y, test_y):\n",
    "    error_rate = []\n",
    "    kvals = range(1,21)  # range of k parameters to test\n",
    "    for i in kvals:\n",
    "        knn = KNeighborsClassifier(n_neighbors=i)\n",
    "        knn.fit(train_x,train_y)\n",
    "        pred_y_i = knn.predict(test_x)\n",
    "        error_rate.append(np.mean(pred_y_i != test_y))\n",
    "    plt.plot(kvals, error_rate, color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "    plt.title('Error Rate vs. K Value')\n",
    "    plt.xlabel('K')\n",
    "    plt.ylabel('Error Rate')\n",
    "    plt.show()\n",
    "    kloc = error_rate.index(min(error_rate))\n",
    "    print('Lowest error is %s occurs at k=%s.' % (error_rate[kloc], kvals[kloc]))\n",
    "    clf = KNeighborsClassifier(kvals[kloc], 'uniform')\n",
    "    clf.fit(train_x, train_y)\n",
    "    \n",
    "    scores = cross_val_score(clf, train_x, train_y, cv=5)\n",
    "    print(\"In-sample accuracy for KNN: %.10f\" % scores.mean())\n",
    "    scores = cross_val_score(clf, test_x, test_y, cv=5) \n",
    "    print(\"Out-of-sample accuracy for KNN: %.10f\" % scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Approach:\n",
    "We wrote our own method to implement the k-Nearest Neighbors algorithm from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Approach 2:\n",
    "# We wrote our own method to implement the k-Nearest Neighbors algorithm from scratch.\n",
    "\n",
    "import math\n",
    "import operator\n",
    "from scipy.stats import mode\n",
    "\n",
    "\n",
    "# Euclidean distance calculation\n",
    "# In order to make predictions we calculate the similarity between any two given data instances. \n",
    "# This is needed so that we can locate the k most similar data instances in the training dataset for a given member of the test dataset and in turn make a prediction.\n",
    "def calculateDistance(instance1, instance2,length):\n",
    "    distance = 0\n",
    "    for x in range(length):\n",
    "        distance += pow((instance1[x] - instance2[x]),2)\n",
    "    return math.sqrt(distance)\n",
    "\n",
    "\n",
    "# Below is the getNeighbors function that returns k most similar neighbors \n",
    "# from the training set for a given test instance (using the already defined euclideanDistance function)\n",
    "def getNeighbors(train_data, train_labels, test_data_single, k):\n",
    "    distances = []\n",
    "    length = train_data.shape[1]\n",
    "    for x in range(len(train_data)):\n",
    "        dist = calculateDistance(test_data_single, train_data[x],length)\n",
    "        distances.append((train_data[x],train_labels[x],dist))\n",
    "    distances.sort(key=operator.itemgetter(2))  #sort based on 3rd item which is distance\n",
    "    neighbors = []\n",
    "    for x in range(k):\n",
    "        neighbors.append(distances[x][1])  #return top k nearest data's label\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "# The funuction below is used for getting the majority voted response from a number of neighbors.\n",
    "def getResponse(neighbors):\n",
    "    classVotes = {}\n",
    "    for x in range(len(neighbors)):\n",
    "        response = neighbors[x]\n",
    "        if response in classVotes:\n",
    "            classVotes[response] += 1\n",
    "        else:\n",
    "            classVotes[response] = 1\n",
    "    sortedVotes = sorted(classVotes.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedVotes[0][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_my_k_nearest_neighbour(train_x, test_x, train_y, test_y):\n",
    "    accuracy = []\n",
    "    print(\"\")\n",
    "    print(\"Use self-written KNeighbors Classifier:\")\n",
    "    for K in range(1,20):\n",
    "        results = []\n",
    "        for i in range(len(test_x)):\n",
    "            test_single = test_x[i]\n",
    "            neighbor = getNeighbors(train_x, train_y, test_single, K)\n",
    "            res = getResponse(neighbor)\n",
    "            results.append(res)\n",
    "        # We evaluate the accuracy of the model by calculating a ratio of the total correct predictions out of all predictions made (the classification accuracy)\n",
    "        correct = 0\n",
    "        wrong = 0\n",
    "        for j in range(len(test_y)):\n",
    "            if(results[j] == test_y[j]):\n",
    "                correct += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "        accuracy.append(float(correct)/(correct+wrong))\n",
    "        print('Correct rate is %s when K = %s.'  % (float(correct)/(correct+wrong),K))\n",
    "    print('Highest correct rate %s occurs at K = %s.' % (max(accuracy), accuracy.index(max(accuracy))+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "old conclusion: \n",
    "\n",
    "A k value that is too low will overfit the data, missing general trends that better distinguish the different classes. Conversely, a k value that is too high will underfit the data and lead to a classification model that creates loose and ill-defined boundaries around key features in the data field.\n",
    "\n",
    "For our datas set, the KNN method is not very successful in differentiating between male and female voices with all features, given the acoustic properties provided.\n",
    "\n",
    "However, K-means result with only two features gives 91.3 % against 59.8% with all features. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
