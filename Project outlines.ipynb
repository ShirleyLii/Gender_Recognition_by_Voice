{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Members:\n",
    "     Kexian Wu\n",
    "     Yingfei Li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For making prediction, we have used 6 algorithms:\n",
    "1. Neural Network\n",
    "\n",
    "2. KNN\n",
    "\n",
    "3. Gaussian Naive Bayes\n",
    "\n",
    "4. Decision Treee\n",
    "\n",
    "5. Random Forest\n",
    "\n",
    "6. SVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Results\n",
    "\n",
    "Below we show our results of each algorithm with 368 train samples and 92 test samples with all features and with 2 features('meanfun', 'IQR') separately.\n",
    "\n",
    "### Baseline \n",
    "50% / 50% (always predict male) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network \n",
    "\n",
    "91.0% / 83.7% (with all features)\n",
    "\n",
    "88.6% / 87.0%  (with 2 features)\n",
    "\n",
    "This model achieves an accuracy of 91.0% on the training set and 83.7% on the test set with all features, and achieves an accuracy of 88.6% on the training set and 87.0% on the test set with all features.\n",
    "\n",
    "We did observe some improvement in performance when we implement KNN with 2 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN\n",
    "59.8% (with all features)\n",
    "\n",
    "91.3% (with 2 features)\n",
    "\n",
    "This model achieves an accuracy of 66.3% on the the test set with all features and achieves an accuracy of 91.3% on the the test set with only two features.\n",
    "\n",
    "We did observe lots of improvement in performance when we implement KNN with 2 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "89.1% / 87% (with all features)\n",
    " \n",
    "89.1% / 89.1% (with 2 features)\n",
    "\n",
    "This model achieves an accuracy of 89.1% on the training set and 87% on the test set with all features and achieves an accuracy of 89.1% on the training set and 89.1% on the test set with all features.\n",
    "\n",
    "We did not observe much improvement in performance when we implement Random Forest with 2 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes\n",
    "81% / 78% (with all features)\n",
    "\n",
    "88% / 87% (with 2 features)\n",
    "\n",
    "This model achieves an accuracy of 81% on the training set and 78% on the test set with all features, and achieves an accuracy of 88% on the training set and 87% on the test set with all features.\n",
    "\n",
    "We did observe some improvement in performance when we implement Gaussian Naive Bayes with 2 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "100% / 80.4% (with all features)\n",
    "\n",
    "100% / 84.8% (with 2 features)\n",
    "\n",
    "\n",
    "This model achieves an accuracy of 100% on the training set and 80.4% on the test set with all features and achieves an accuracy of 100% on the training set and 84.8% on the test set with relevant features. Deleting irrelevant features improved the out-of-sample accuracy, but the in sample accuracy decreased. This is reasonable, as overfitting mainly causes by the model's attemption of fitting the inaccurate samples in the training set. The decrease in in-sample accuracy means that these inaccurate samples are excluded during training.\n",
    "\n",
    "We did observe some improvement in performance when we implement Decision Tree with 2 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "\n",
    "89.7% / 85.9% (with all features)\n",
    "\n",
    "88.0% / 84.8% (with 2 features)\n",
    "\n",
    "This model achieves an accuracy of 92% on the training set and 82% on the test set with all features and achieves an accuracy of 91% on the training set and 84% on the test set with relevant features\n",
    "\n",
    "We did not observe any improvement in performance when we implement SVM with 2 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What We found:\n",
    "\n",
    "With all features, best result results (accuracy on test set) we have in Random Forest, SVM, Decision Tree, Gaussian Naive Bayes, KNN.\n",
    "\n",
    "\n",
    "But in case with only 2 features(‘meanfun’, ‘IQR’) without feature scaling results are more precise. We found that feature scaling improves results, except SVM. With only 2 features, best result results (accuracy on test set) we have in KNN, Random Forest, Neural Network, Gaussian Naive Bayes, Decision Tree, SVM. \n",
    "\n",
    "\n",
    "Naive Bayers, for example, gives 87% against 78% with all features. That is because algorithm is based on suggestion of independence. The most significant improvement is in KNN without feature scaling, it grows up from ~59.8% to 91.3%. The reason is the same, less quantity of features. \n",
    "\n",
    "Generally, results of partial dataset do not lose to full, but sometimes gives even better results. Hence, in some cases we can use only 2 features, instead of 20 features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of learning and practicing, we developed our own functions for most of the machine learning methods we used in this task, and compared our results to the results obtained by using Sklearn/Keras package. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Classifier(20 features）|Using Built-in Algorithm| Using Self-Written Algorithm\n",
    "-------------------|---------------------------| ----------------------------\n",
    "Neural Network| 83.7%| 81.5%\n",
    "KNN | 54.3% | 59.8%\n",
    "Gaussian Naive Bayes | 78.3% | 78.3%\n",
    "Decision Tree|78.3% |  80.4%\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are some reasons that we cannot expect 100% result:\n",
    "\n",
    "1) speech defects. Percent of these people is quite minor, but we should consider it. \n",
    "\n",
    "2) Models provide us quite high results with special dataset. In real life, the conditions of recording matter. \n",
    "   So, we should take into account many other    features.\n",
    "\n",
    "We believe that removing useless features and adding other one is the most effective way of improving our results with selected models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To improve results we have used feature scaling. Also, there are a few other ways:\n",
    "    \n",
    "1) get more accurate records of voices. It would minimize incorrect predictions;\n",
    "\n",
    "2) remove some useless features and add another features, as in real world there are often much larger variety of data;\n",
    "\n",
    "3) there should be right chosen environment and conditions of recording since intonation can also influences results;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### With given dataset, we can predict gender with possibility in 91.3% with KNN model with 2 features, meanfun and IQR.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "Even though the accuracy of the result obtained by using our ow data and functions are lower than using Kaggle’s data and Sklearn package, we got exposed to the process of preprocessing raw data. For the sake of keeping the completeness of this task, we consider this experience very meaningful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributions: \n",
    "    \n",
    "    Voices Collection (Shell Script)   - \n",
    "    \n",
    "    R Srcript                          - \n",
    "\n",
    "    01 Data Exploration                - \n",
    "    \n",
    "    02 Neural Network (TensorFlow)     - \n",
    "    \n",
    "    03 KNN                             - \n",
    "    \n",
    "    04 Gaussian Naive Bayes            - \n",
    "    \n",
    "    05 Decision Tree                   - \n",
    "    \n",
    "    06 Random Forest (ensemble method) - \n",
    "    \n",
    "    07 SVM                             - \n",
    "    \n",
    "    08 Models Accuracy                 - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References\n",
    "\n",
    "VoxForge Speech Corpus\n",
    "\n",
    "Website: http://www.repository.voxforge1.org/downloads/SpeechCorpus/Trunk/Audio/Main/8kHz_16bit/\n",
    "\n",
    "Gender Recognition by Voice(where our project topic comes from)\n",
    "\n",
    "Website: https://www.kaggle.com/primaryobjects/voicegender\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
