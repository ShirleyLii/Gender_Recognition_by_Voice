{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6b155277a924>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np  \n",
    "import random\n",
    "from sklearn.model_selection import train_test_split  \n",
    "# import tensorflow as tf \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Data preparation\n",
    "# path = '/Users/Kassi/Desktop/Gender_Recognition_by_Voice/voice.csv'\n",
    "# voice_data = pd.read_csv(path)\n",
    "# voice_data = voice_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # There are multiple steps we will take in this section to transform the original data into format which we can easily plug inside tensorflow's tensors.\n",
    "# # Creating 1-hot vector from the original labels\n",
    "# # Randomly shuffle data\n",
    "# # Create train/test datasets\n",
    "\n",
    "# voices = voice_data[:, :-1] \n",
    "# labels = voice_data[:, -1:]\n",
    "\n",
    "# # Classification: label(gender) = {male, female} â†’ [1.0, 0.0], [0.0, 1.0]  \n",
    "# labels_tmp = []  \n",
    "# for label in labels:  \n",
    "#     tmp = []  \n",
    "#     if label[0] == 'male':  \n",
    "#         tmp = [1.0, 0.0]  \n",
    "#     else:   \n",
    "#         tmp = [0.0, 1.0]  \n",
    "#     labels_tmp.append(tmp)  \n",
    "# labels = np.array(labels_tmp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # randomly shuffle data\n",
    "# voices_tmp = []  \n",
    "# lables_tmp = []  \n",
    "# index_shuf = range(len(voices)) \n",
    "# random.shuffle(index_shuf) \n",
    "# for i in index_shuf:  \n",
    "#     voices_tmp.append(voices[i])  \n",
    "#     lables_tmp.append(labels[i])  \n",
    "# voices = np.array(voices_tmp)  \n",
    "# labels = np.array(lables_tmp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_x, test_x, train_y, test_y = train_test_split(voices, labels, test_size=0.2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we are all set to get started with creating the tensorflow graph. \n",
    "# It is important to understand that after this step there is no actual computation being done by tensorflow. \n",
    "# It just creates a lazy graph according to the nodes we create in the neural_network method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Learning Parameters\n",
    "# rate   =   0.0010  # training rate\n",
    "# epochs =  2000     # number of full training cycles (I select this number based on the trend of cost)\n",
    "# banch_size  = 68   # number of data points to train per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Network Parameters\n",
    "# n_hidden_1 = 512  # number of nodes in hidden layer 1\n",
    "# n_hidden_2 = 512  # number of nodes in hidden layer 2\n",
    "# n_input    = voices.shape[-1]  # 20\n",
    "# n_classes  = 2\n",
    "# n_samples  = len(voices)  # 460"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X = tf.placeholder('float', [None,n_input])\n",
    "# Y = tf.placeholder('float', [None,n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# weights = {\n",
    "#     'w1' : tf.Variable(tf.random_normal([n_input,    n_hidden_1])),\n",
    "#     'w2' : tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "#     'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes ]))\n",
    "# }\n",
    "\n",
    "# biases = {\n",
    "#     'b1' : tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "#     'b2' : tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "#     'out': tf.Variable(tf.random_normal([n_classes ]))\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def neural_network(X, weights, biases):  \n",
    "#     # Hidden Layer 1\n",
    "#     layer1 = tf.matmul(X, weights['w1'])\n",
    "#     layer1 = tf.add(layer1, biases['b1'])\n",
    "#     layer1 = tf.nn.softmax(layer1)\n",
    "    \n",
    "#     # Hidden Layer 2\n",
    "#     layer2 = tf.matmul(layer1, weights['w2'])\n",
    "#     layer2 = tf.add(layer2, biases['b2'])\n",
    "#     layer2 = tf.nn.softmax(layer2)\n",
    "    \n",
    "#     # Output Layer\n",
    "#     output = tf.matmul(layer2, weights['out'])\n",
    "#     output = tf.add(output, biases['out'])\n",
    "#     output = tf.nn.softmax(output)\n",
    "    \n",
    "#     return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # In this section we define the optimizer and loss functions. \n",
    "# # We also define our notion of correct and incorrect prediction and what we mean by accuracy.\n",
    "\n",
    "# def train_neural_network(train_x,train_y,test_x,test_y):\n",
    "#     model = neural_network(X, weights, biases)\n",
    "#     f_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
    "#     f_optimizer = tf.train.AdamOptimizer(learning_rate=rate).minimize(f_cost)\n",
    "#     with tf.Session() as s:\n",
    "#         s.run(tf.global_variables_initializer())\n",
    "#         for epoch in range(epochs):\n",
    "#             cost_avg = 0.\n",
    "#             batch_total = len(train_x) // banch_size\n",
    "#             for banch in range(batch_total):\n",
    "#                 voice_banch = train_x[banch*banch_size:(banch+1)*(banch_size)]  \n",
    "#                 label_banch = train_y[banch*banch_size:(banch+1)*(banch_size)]        \n",
    "#                 _, cost = s.run([f_optimizer,f_cost], feed_dict={X:voice_banch, Y: label_banch})        \n",
    "#                 cost_avg += cost / batch_total\n",
    "        \n",
    "#             print('Epoch {}: cost={:.4f}'.format(epoch+1, cost_avg))\n",
    "    \n",
    "#         # testing\n",
    "        \n",
    "#         # This gives us a list of booleans\n",
    "#         prediction = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))  \n",
    "#         # We cast to floating point numbers and then take the mean\n",
    "#         accuracy = tf.reduce_mean(tf.cast(prediction, dtype=tf.float32))  \n",
    "#         accuracy = s.run(accuracy, feed_dict={X: train_x, Y: train_y})  \n",
    "#         print('In-sample accuracy in Neural Network: %s'  % (accuracy)) \n",
    "        \n",
    "#         accuracy = tf.reduce_mean(tf.cast(prediction, dtype=tf.float32)) \n",
    "#         accuracy = s.run(accuracy, feed_dict={X: test_x, Y: test_y})  \n",
    "#         print('Out-of-sample accuracy in Neural Network: %s'  % (accuracy)) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Execute tf graph\n",
    "# # Now that the graph is set up, we can run it\n",
    "\n",
    "# print('Accuracy in Neural Network with all features:')\n",
    "# train_neural_network(train_x,train_y,test_x,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are wondering if there is a big difference in the results when used Keras deep learning library with a TensorFlow backend vs. our own step-by-step algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Importing Libraries for ANN\n",
    "# import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "\n",
    "# #Initialisng the ANN\n",
    "# classifier = Sequential()\n",
    "\n",
    "# #Input Layer and First Hidden Layer\n",
    "# classifier.add(Dense(units = 512, activation = 'softmax', input_dim = train_x.shape[1]))\n",
    "\n",
    "# #Adding the Second hidden layer\n",
    "# classifier.add(Dense(units = 512, activation = 'softmax'))\n",
    "\n",
    "# # Adding the output layer\n",
    "# classifier.add(Dense(units = 2, activation = 'softmax'))\n",
    "\n",
    "# #Compiling the ANN\n",
    "# classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Fitting ANN to the training set\n",
    "# classifier.fit(train_x, train_y, batch_size = 68, epochs = 2000, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Predicting the Test set results\n",
    "# pred_train_y = classifier.predict(train_x)\n",
    "# pred_train_y = (pred_train_y > 0.5)\n",
    "# # pred_train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels_tmp = []  \n",
    "# for label in pred_train_y:  \n",
    "#     tmp = []  \n",
    "#     if label[0] == True:  \n",
    "#         tmp = [1,0]  \n",
    "#     else:   \n",
    "#         tmp = [0,1]  \n",
    "#     labels_tmp.append(tmp)  \n",
    "# new_result = np.array(labels_tmp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "# print('In-sample accuracy in Neural Network using Keras package (with all features):%s' % (accuracy_score(train_y, new_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Predicting the Test set results\n",
    "# pred_test_y = classifier.predict(test_x)\n",
    "# pred_test_y = (pred_test_y > 0.5)\n",
    "# # pred_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels_tmp = []  \n",
    "# for label in pred_test_y:  \n",
    "#     tmp = []  \n",
    "#     if label[0] == True:  \n",
    "#         tmp = [1,0]  \n",
    "#     else:   \n",
    "#         tmp = [0,1]  \n",
    "#     labels_tmp.append(tmp)  \n",
    "# new_result = np.array(labels_tmp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Out-of-sample accuracy in Neural Network using Keras package (with all features):%s' % (accuracy_score(test_y, new_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Is it enough of these two features to make predictions? \n",
    "# ## Neural Network with 2 features ('meanfun', 'IQR')\n",
    "# train_x = train_x[:,[5,12]]\n",
    "# test_x = test_x[:,[5,12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # It seems we can better result when using Keras deep learning library, \n",
    "# # so we will continue to use this package to do prediction.\n",
    "\n",
    "# #Initialisng the ANN\n",
    "# classifier = Sequential()\n",
    "\n",
    "# #Input Layer and First Hidden Layer\n",
    "# classifier.add(Dense(units = 512, activation = 'softmax', input_dim = train_x.shape[1]))\n",
    "\n",
    "# #Adding the Second hidden layer\n",
    "# classifier.add(Dense(units = 512, activation = 'softmax'))\n",
    "\n",
    "# # Adding the output layer\n",
    "# classifier.add(Dense(units = 2, activation = 'softmax'))\n",
    "\n",
    "# #Compiling the ANN\n",
    "# classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Fitting ANN to the training set\n",
    "# classifier.fit(train_x, train_y, batch_size = 68, epochs = 2000, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Predicting the Train set results\n",
    "# pred_y = classifier.predict(train_x)\n",
    "# pred_y = (pred_y > 0.5)\n",
    "# # pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels_tmp = []  \n",
    "# for label in pred_y:  \n",
    "#     tmp = []  \n",
    "#     if label[0] == True:  \n",
    "#         tmp = [1,0]  \n",
    "#     else:   \n",
    "#         tmp = [0,1]  \n",
    "#     labels_tmp.append(tmp)  \n",
    "# new_result = np.array(labels_tmp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('In-sample accuracy in Neural Network using Keras package (with 2 features):%s' % (accuracy_score(train_y, new_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Predicting the Test set results\n",
    "# pred_y = classifier.predict(test_x)\n",
    "# pred_y = (pred_y > 0.5)\n",
    "# # pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels_tmp = []  \n",
    "# for label in pred_y:  \n",
    "#     tmp = []  \n",
    "#     if label[0] == True:  \n",
    "#         tmp = [1,0]  \n",
    "#     else:   \n",
    "#         tmp = [0,1]  \n",
    "#     labels_tmp.append(tmp)  \n",
    "# new_result = np.array(labels_tmp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Out-of-sample accuracy in Neural Network using Keras package (with 2 features):%s' % (accuracy_score(test_y, new_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In case with only 2 features(â€˜meanfunâ€™, â€˜IQRâ€™) without feature scaling results are more precise. \n",
    "\n",
    "Neural Network with only two features gives 87.0% against 83.7% with all features. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
