{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np  \n",
    "import random\n",
    "from sklearn.model_selection import train_test_split  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Installing Theano\n",
    "# pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git\n",
    "\n",
    "# Installing Tensorflow\n",
    "# Install Tensorflow from the website: https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html\n",
    "\n",
    "# Installing Keras\n",
    "# pip install --upgrade keras\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%store -r pca_train_x\n",
    "%store -r pca_test_x\n",
    "%store -r pca13_train_x\n",
    "%store -r pca13_test_x\n",
    "\n",
    "%store -r train_x\n",
    "%store -r test_x\n",
    "%store -r train_y\n",
    "%store -r test_y\n",
    "\n",
    "%store -r train_x_two_features\n",
    "%store -r test_x_two_features\n",
    "%store -r train_y_two_features\n",
    "%store -r test_y_two_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.5798 - acc: 0.7980\n",
      "Epoch 2/100\n",
      "2376/2376 [==============================] - 1s 302us/step - loss: 0.2429 - acc: 0.8939\n",
      "Epoch 3/100\n",
      "2376/2376 [==============================] - 1s 248us/step - loss: 0.1430 - acc: 0.9343\n",
      "Epoch 4/100\n",
      "2376/2376 [==============================] - 1s 242us/step - loss: 0.0992 - acc: 0.9714\n",
      "Epoch 5/100\n",
      "2376/2376 [==============================] - 1s 230us/step - loss: 0.0871 - acc: 0.9710\n",
      "Epoch 6/100\n",
      "2376/2376 [==============================] - 1s 226us/step - loss: 0.0815 - acc: 0.9756\n",
      "Epoch 7/100\n",
      "2376/2376 [==============================] - 1s 243us/step - loss: 0.0790 - acc: 0.9764\n",
      "Epoch 8/100\n",
      "2376/2376 [==============================] - 1s 234us/step - loss: 0.0766 - acc: 0.9781\n",
      "Epoch 9/100\n",
      "2376/2376 [==============================] - 1s 234us/step - loss: 0.0751 - acc: 0.9777\n",
      "Epoch 10/100\n",
      "2376/2376 [==============================] - 1s 229us/step - loss: 0.0744 - acc: 0.9790\n",
      "Epoch 11/100\n",
      "2376/2376 [==============================] - 1s 230us/step - loss: 0.0732 - acc: 0.9781\n",
      "Epoch 12/100\n",
      "2376/2376 [==============================] - 1s 240us/step - loss: 0.0719 - acc: 0.9790\n",
      "Epoch 13/100\n",
      "2376/2376 [==============================] - 1s 227us/step - loss: 0.0714 - acc: 0.9785\n",
      "Epoch 14/100\n",
      "2376/2376 [==============================] - 1s 238us/step - loss: 0.0696 - acc: 0.9790\n",
      "Epoch 15/100\n",
      "2376/2376 [==============================] - 1s 230us/step - loss: 0.0689 - acc: 0.9798\n",
      "Epoch 16/100\n",
      "2376/2376 [==============================] - 1s 252us/step - loss: 0.0678 - acc: 0.9802\n",
      "Epoch 17/100\n",
      "2376/2376 [==============================] - 1s 291us/step - loss: 0.0679 - acc: 0.9798\n",
      "Epoch 18/100\n",
      "2376/2376 [==============================] - 1s 332us/step - loss: 0.0671 - acc: 0.9802\n",
      "Epoch 19/100\n",
      "2376/2376 [==============================] - 1s 346us/step - loss: 0.0670 - acc: 0.9781\n",
      "Epoch 20/100\n",
      "2376/2376 [==============================] - 1s 316us/step - loss: 0.0652 - acc: 0.9819\n",
      "Epoch 21/100\n",
      "2376/2376 [==============================] - 1s 292us/step - loss: 0.0656 - acc: 0.9790\n",
      "Epoch 22/100\n",
      "2376/2376 [==============================] - 1s 299us/step - loss: 0.0640 - acc: 0.9815\n",
      "Epoch 23/100\n",
      "2376/2376 [==============================] - 1s 286us/step - loss: 0.0640 - acc: 0.9802\n",
      "Epoch 24/100\n",
      "2376/2376 [==============================] - 1s 260us/step - loss: 0.0631 - acc: 0.9811\n",
      "Epoch 25/100\n",
      "2376/2376 [==============================] - 1s 244us/step - loss: 0.0628 - acc: 0.9806\n",
      "Epoch 26/100\n",
      "2376/2376 [==============================] - 1s 229us/step - loss: 0.0627 - acc: 0.9811\n",
      "Epoch 27/100\n",
      "2376/2376 [==============================] - 1s 226us/step - loss: 0.0623 - acc: 0.9815\n",
      "Epoch 28/100\n",
      "2376/2376 [==============================] - 1s 237us/step - loss: 0.0613 - acc: 0.9815\n",
      "Epoch 29/100\n",
      "2376/2376 [==============================] - 1s 243us/step - loss: 0.0606 - acc: 0.9811\n",
      "Epoch 30/100\n",
      "2376/2376 [==============================] - 1s 227us/step - loss: 0.0619 - acc: 0.9806\n",
      "Epoch 31/100\n",
      "2376/2376 [==============================] - 1s 225us/step - loss: 0.0605 - acc: 0.9819\n",
      "Epoch 32/100\n",
      "2376/2376 [==============================] - 1s 242us/step - loss: 0.0599 - acc: 0.9811\n",
      "Epoch 33/100\n",
      "2376/2376 [==============================] - 1s 225us/step - loss: 0.0585 - acc: 0.9823\n",
      "Epoch 34/100\n",
      "2376/2376 [==============================] - 1s 250us/step - loss: 0.0587 - acc: 0.9827\n",
      "Epoch 35/100\n",
      "2376/2376 [==============================] - 1s 265us/step - loss: 0.0579 - acc: 0.9832\n",
      "Epoch 36/100\n",
      "2376/2376 [==============================] - 1s 280us/step - loss: 0.0574 - acc: 0.9832\n",
      "Epoch 37/100\n",
      "2376/2376 [==============================] - 1s 231us/step - loss: 0.0580 - acc: 0.9832\n",
      "Epoch 38/100\n",
      "2376/2376 [==============================] - 1s 242us/step - loss: 0.0576 - acc: 0.9823\n",
      "Epoch 39/100\n",
      "2376/2376 [==============================] - 1s 249us/step - loss: 0.0568 - acc: 0.9832\n",
      "Epoch 40/100\n",
      "2376/2376 [==============================] - 1s 247us/step - loss: 0.0556 - acc: 0.9848\n",
      "Epoch 41/100\n",
      "2376/2376 [==============================] - 1s 244us/step - loss: 0.0558 - acc: 0.9836\n",
      "Epoch 42/100\n",
      "2376/2376 [==============================] - 1s 255us/step - loss: 0.0549 - acc: 0.9840\n",
      "Epoch 43/100\n",
      "2376/2376 [==============================] - 1s 274us/step - loss: 0.0553 - acc: 0.9832\n",
      "Epoch 44/100\n",
      "2376/2376 [==============================] - 1s 223us/step - loss: 0.0533 - acc: 0.9832\n",
      "Epoch 45/100\n",
      "2376/2376 [==============================] - 1s 232us/step - loss: 0.0536 - acc: 0.9827\n",
      "Epoch 46/100\n",
      "2376/2376 [==============================] - 1s 236us/step - loss: 0.0529 - acc: 0.9844\n",
      "Epoch 47/100\n",
      "2376/2376 [==============================] - 1s 253us/step - loss: 0.0514 - acc: 0.9836\n",
      "Epoch 48/100\n",
      "2376/2376 [==============================] - 1s 240us/step - loss: 0.0525 - acc: 0.9836\n",
      "Epoch 49/100\n",
      "2376/2376 [==============================] - 1s 268us/step - loss: 0.0514 - acc: 0.9844\n",
      "Epoch 50/100\n",
      "2376/2376 [==============================] - 1s 299us/step - loss: 0.0505 - acc: 0.9844\n",
      "Epoch 51/100\n",
      "2376/2376 [==============================] - 1s 298us/step - loss: 0.0501 - acc: 0.9844 0s - loss: 0.0354 - \n",
      "Epoch 52/100\n",
      "2376/2376 [==============================] - 1s 239us/step - loss: 0.0499 - acc: 0.9827\n",
      "Epoch 53/100\n",
      "2376/2376 [==============================] - 1s 241us/step - loss: 0.0493 - acc: 0.9848\n",
      "Epoch 54/100\n",
      "2376/2376 [==============================] - 1s 261us/step - loss: 0.0489 - acc: 0.9848\n",
      "Epoch 55/100\n",
      "2376/2376 [==============================] - 1s 239us/step - loss: 0.0482 - acc: 0.9848\n",
      "Epoch 56/100\n",
      "2376/2376 [==============================] - 1s 230us/step - loss: 0.0479 - acc: 0.9853\n",
      "Epoch 57/100\n",
      "2376/2376 [==============================] - 1s 220us/step - loss: 0.0469 - acc: 0.9844\n",
      "Epoch 58/100\n",
      "2376/2376 [==============================] - 1s 254us/step - loss: 0.0468 - acc: 0.9853\n",
      "Epoch 59/100\n",
      "2376/2376 [==============================] - 1s 258us/step - loss: 0.0481 - acc: 0.9848\n",
      "Epoch 60/100\n",
      "2376/2376 [==============================] - 1s 228us/step - loss: 0.0458 - acc: 0.9848\n",
      "Epoch 61/100\n",
      "2376/2376 [==============================] - 1s 249us/step - loss: 0.0464 - acc: 0.9848\n",
      "Epoch 62/100\n",
      "2376/2376 [==============================] - 1s 235us/step - loss: 0.0455 - acc: 0.9848\n",
      "Epoch 63/100\n",
      "2376/2376 [==============================] - 1s 245us/step - loss: 0.0458 - acc: 0.9848 0s - loss: 0.0412 - ac\n",
      "Epoch 64/100\n",
      "2376/2376 [==============================] - 1s 234us/step - loss: 0.0448 - acc: 0.9848\n",
      "Epoch 65/100\n",
      "2376/2376 [==============================] - 1s 238us/step - loss: 0.0441 - acc: 0.9848\n",
      "Epoch 66/100\n",
      "2376/2376 [==============================] - 1s 250us/step - loss: 0.0450 - acc: 0.9840\n",
      "Epoch 67/100\n",
      "2376/2376 [==============================] - 1s 233us/step - loss: 0.0441 - acc: 0.9848\n",
      "Epoch 68/100\n",
      "2376/2376 [==============================] - 1s 231us/step - loss: 0.0444 - acc: 0.9861\n",
      "Epoch 69/100\n",
      "2376/2376 [==============================] - 1s 239us/step - loss: 0.0435 - acc: 0.9857\n",
      "Epoch 70/100\n",
      "2376/2376 [==============================] - 1s 247us/step - loss: 0.0432 - acc: 0.9844\n",
      "Epoch 71/100\n",
      "2376/2376 [==============================] - 1s 256us/step - loss: 0.0431 - acc: 0.9848\n",
      "Epoch 72/100\n",
      "2376/2376 [==============================] - 1s 249us/step - loss: 0.0421 - acc: 0.9857\n",
      "Epoch 73/100\n",
      "2376/2376 [==============================] - 1s 257us/step - loss: 0.0422 - acc: 0.9865\n",
      "Epoch 74/100\n",
      "2376/2376 [==============================] - 1s 251us/step - loss: 0.0417 - acc: 0.9853\n",
      "Epoch 75/100\n",
      "2376/2376 [==============================] - 1s 244us/step - loss: 0.0417 - acc: 0.9861\n",
      "Epoch 76/100\n",
      "2376/2376 [==============================] - 1s 253us/step - loss: 0.0415 - acc: 0.9853\n",
      "Epoch 77/100\n",
      "2376/2376 [==============================] - 1s 305us/step - loss: 0.0404 - acc: 0.9853\n",
      "Epoch 78/100\n",
      "2376/2376 [==============================] - ETA: 0s - loss: 0.0404 - acc: 0.985 - 1s 305us/step - loss: 0.0408 - acc: 0.9848\n",
      "Epoch 79/100\n",
      "2376/2376 [==============================] - 1s 246us/step - loss: 0.0403 - acc: 0.9861\n",
      "Epoch 80/100\n",
      "2376/2376 [==============================] - 1s 252us/step - loss: 0.0401 - acc: 0.9848\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2376/2376 [==============================] - 1s 229us/step - loss: 0.0395 - acc: 0.9870\n",
      "Epoch 82/100\n",
      "2376/2376 [==============================] - 0s 210us/step - loss: 0.0402 - acc: 0.9865\n",
      "Epoch 83/100\n",
      "2376/2376 [==============================] - 1s 220us/step - loss: 0.0391 - acc: 0.9861\n",
      "Epoch 84/100\n",
      "2376/2376 [==============================] - 0s 210us/step - loss: 0.0379 - acc: 0.9874\n",
      "Epoch 85/100\n",
      "2376/2376 [==============================] - 0s 210us/step - loss: 0.0392 - acc: 0.9861\n",
      "Epoch 86/100\n",
      "2376/2376 [==============================] - 0s 207us/step - loss: 0.0384 - acc: 0.9865\n",
      "Epoch 87/100\n",
      "2376/2376 [==============================] - 1s 214us/step - loss: 0.0386 - acc: 0.9861\n",
      "Epoch 88/100\n",
      "2376/2376 [==============================] - 1s 220us/step - loss: 0.0385 - acc: 0.9865\n",
      "Epoch 89/100\n",
      "2376/2376 [==============================] - 1s 214us/step - loss: 0.0386 - acc: 0.9870\n",
      "Epoch 90/100\n",
      "2376/2376 [==============================] - 1s 215us/step - loss: 0.0377 - acc: 0.9870\n",
      "Epoch 91/100\n",
      "2376/2376 [==============================] - 0s 206us/step - loss: 0.0369 - acc: 0.9865\n",
      "Epoch 92/100\n",
      "2376/2376 [==============================] - 0s 208us/step - loss: 0.0384 - acc: 0.9853\n",
      "Epoch 93/100\n",
      "2376/2376 [==============================] - 1s 211us/step - loss: 0.0368 - acc: 0.9882\n",
      "Epoch 94/100\n",
      "2376/2376 [==============================] - 1s 213us/step - loss: 0.0361 - acc: 0.9878 0s - loss: 0.0417 - acc: 0.984 - ETA: 0s - loss: 0.0348 - acc:\n",
      "Epoch 95/100\n",
      "2376/2376 [==============================] - 1s 215us/step - loss: 0.0364 - acc: 0.9861\n",
      "Epoch 96/100\n",
      "2376/2376 [==============================] - 1s 221us/step - loss: 0.0366 - acc: 0.9870\n",
      "Epoch 97/100\n",
      "2376/2376 [==============================] - 1s 212us/step - loss: 0.0359 - acc: 0.9874\n",
      "Epoch 98/100\n",
      "2376/2376 [==============================] - 0s 205us/step - loss: 0.0369 - acc: 0.9857\n",
      "Epoch 99/100\n",
      "2376/2376 [==============================] - 1s 215us/step - loss: 0.0357 - acc: 0.9870\n",
      "Epoch 100/100\n",
      "2376/2376 [==============================] - 1s 211us/step - loss: 0.0346 - acc: 0.9895\n",
      "0.976010101010101\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOEjMzKwmDhIzM6vJ/weEC13qsbXpAgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a162a97f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_x_two_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-165-1bab05e82062>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mrun_ANN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mrun_ANN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x_two_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x_two_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y_two_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y_two_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mrun_ANN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca13_train_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpca13_test_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_x_two_features' is not defined"
     ]
    }
   ],
   "source": [
    "# import the keras lib and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import sklearn.metrics as metrics\n",
    "from keras.callbacks import History \n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "def run_ANN(train_x,test_x,train_y,test_y):\n",
    "    #Initializing ANN\n",
    "    clf = Sequential()\n",
    "    hist = History()\n",
    "    clf.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = len(train_x[1])))\n",
    "    # Adding the second hidden layer\n",
    "    clf.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "    # Adding the output layer\n",
    "    clf.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "    clf.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    clf.fit(train_x, train_y, batch_size = 10, nb_epoch = 100 ,callbacks = [hist])\n",
    "    pred_y = clf.predict(test_x)\n",
    "    pred_y = (pred_y > 0.5)\n",
    "    \n",
    "        \n",
    "    print(metrics.accuracy_score(pred_y,test_y))\n",
    "\n",
    "    plt.plot(hist.history['acc'], color = 'red')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "        \n",
    "run_ANN(train_x,test_x,train_y,test_y)\n",
    "run_ANN(train_x_two_features, test_x_two_features, train_y_two_features, test_y_two_features)\n",
    "run_ANN(pca13_train_x, pca13_test_x, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Data preparation\n",
    "# path = '/Users/Kassi/Desktop/Gender_Recognition_by_Voice/voice.csv'\n",
    "# voice_data = pd.read_csv(path)\n",
    "# voice_data = voice_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # There are multiple steps we will take in this section to transform the original data into format which we can easily plug inside tensorflow's tensors.\n",
    "# # Creating 1-hot vector from the original labels\n",
    "# # Randomly shuffle data\n",
    "# # Create train/test datasets\n",
    "\n",
    "# voices = voice_data[:, :-1] \n",
    "# labels = voice_data[:, -1:]\n",
    "\n",
    "# # Classification: label(gender) = {male, female} → [1.0, 0.0], [0.0, 1.0]  \n",
    "# labels_tmp = []  \n",
    "# for label in labels:  \n",
    "#     tmp = []  \n",
    "#     if label[0] == 'male':  \n",
    "#         tmp = [1.0, 0.0]  \n",
    "#     else:   \n",
    "#         tmp = [0.0, 1.0]  \n",
    "#     labels_tmp.append(tmp)  \n",
    "# labels = np.array(labels_tmp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # randomly shuffle data\n",
    "# voices_tmp = []  \n",
    "# lables_tmp = []  \n",
    "# index_shuf = range(len(voices)) \n",
    "# random.shuffle(index_shuf) \n",
    "# for i in index_shuf:  \n",
    "#     voices_tmp.append(voices[i])  \n",
    "#     lables_tmp.append(labels[i])  \n",
    "# voices = np.array(voices_tmp)  \n",
    "# labels = np.array(lables_tmp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_x, test_x, train_y, test_y = train_test_split(voices, labels, test_size=0.2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we are all set to get started with creating the tensorflow graph. \n",
    "# It is important to understand that after this step there is no actual computation being done by tensorflow. \n",
    "# It just creates a lazy graph according to the nodes we create in the neural_network method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Learning Parameters\n",
    "# rate   =   0.0010  # training rate\n",
    "# epochs =  2000     # number of full training cycles (I select this number based on the trend of cost)\n",
    "# banch_size  = 68   # number of data points to train per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Network Parameters\n",
    "# n_hidden_1 = 512  # number of nodes in hidden layer 1\n",
    "# n_hidden_2 = 512  # number of nodes in hidden layer 2\n",
    "# n_input    = voices.shape[-1]  # 20\n",
    "# n_classes  = 2\n",
    "# n_samples  = len(voices)  # 460"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X = tf.placeholder('float', [None,n_input])\n",
    "# Y = tf.placeholder('float', [None,n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# weights = {\n",
    "#     'w1' : tf.Variable(tf.random_normal([n_input,    n_hidden_1])),\n",
    "#     'w2' : tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "#     'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes ]))\n",
    "# }\n",
    "\n",
    "# biases = {\n",
    "#     'b1' : tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "#     'b2' : tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "#     'out': tf.Variable(tf.random_normal([n_classes ]))\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def neural_network(X, weights, biases):  \n",
    "#     # Hidden Layer 1\n",
    "#     layer1 = tf.matmul(X, weights['w1'])\n",
    "#     layer1 = tf.add(layer1, biases['b1'])\n",
    "#     layer1 = tf.nn.softmax(layer1)\n",
    "    \n",
    "#     # Hidden Layer 2\n",
    "#     layer2 = tf.matmul(layer1, weights['w2'])\n",
    "#     layer2 = tf.add(layer2, biases['b2'])\n",
    "#     layer2 = tf.nn.softmax(layer2)\n",
    "    \n",
    "#     # Output Layer\n",
    "#     output = tf.matmul(layer2, weights['out'])\n",
    "#     output = tf.add(output, biases['out'])\n",
    "#     output = tf.nn.softmax(output)\n",
    "    \n",
    "#     return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # In this section we define the optimizer and loss functions. \n",
    "# # We also define our notion of correct and incorrect prediction and what we mean by accuracy.\n",
    "\n",
    "# def train_neural_network(train_x,train_y,test_x,test_y):\n",
    "#     model = neural_network(X, weights, biases)\n",
    "#     f_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
    "#     f_optimizer = tf.train.AdamOptimizer(learning_rate=rate).minimize(f_cost)\n",
    "#     with tf.Session() as s:\n",
    "#         s.run(tf.global_variables_initializer())\n",
    "#         for epoch in range(epochs):\n",
    "#             cost_avg = 0.\n",
    "#             batch_total = len(train_x) // banch_size\n",
    "#             for banch in range(batch_total):\n",
    "#                 voice_banch = train_x[banch*banch_size:(banch+1)*(banch_size)]  \n",
    "#                 label_banch = train_y[banch*banch_size:(banch+1)*(banch_size)]        \n",
    "#                 _, cost = s.run([f_optimizer,f_cost], feed_dict={X:voice_banch, Y: label_banch})        \n",
    "#                 cost_avg += cost / batch_total\n",
    "        \n",
    "#             print('Epoch {}: cost={:.4f}'.format(epoch+1, cost_avg))\n",
    "    \n",
    "#         # testing\n",
    "        \n",
    "#         # This gives us a list of booleans\n",
    "#         prediction = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))  \n",
    "#         # We cast to floating point numbers and then take the mean\n",
    "#         accuracy = tf.reduce_mean(tf.cast(prediction, dtype=tf.float32))  \n",
    "#         accuracy = s.run(accuracy, feed_dict={X: train_x, Y: train_y})  \n",
    "#         print('In-sample accuracy in Neural Network: %s'  % (accuracy)) \n",
    "        \n",
    "#         accuracy = tf.reduce_mean(tf.cast(prediction, dtype=tf.float32)) \n",
    "#         accuracy = s.run(accuracy, feed_dict={X: test_x, Y: test_y})  \n",
    "#         print('Out-of-sample accuracy in Neural Network: %s'  % (accuracy)) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## Execute tf graph\n",
    "# # Now that the graph is set up, we can run it\n",
    "\n",
    "# print('Accuracy in Neural Network with all features:')\n",
    "# train_neural_network(train_x,train_y,test_x,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are wondering if there is a big difference in the results when used Keras deep learning library with a TensorFlow backend vs. our own step-by-step algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Importing Libraries for ANN\n",
    "# import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "\n",
    "# #Initialisng the ANN\n",
    "# classifier = Sequential()\n",
    "\n",
    "# #Input Layer and First Hidden Layer\n",
    "# classifier.add(Dense(units = 512, activation = 'softmax', input_dim = train_x.shape[1]))\n",
    "\n",
    "# #Adding the Second hidden layer\n",
    "# classifier.add(Dense(units = 512, activation = 'softmax'))\n",
    "\n",
    "# # Adding the output layer\n",
    "# classifier.add(Dense(units = 2, activation = 'softmax'))\n",
    "\n",
    "# #Compiling the ANN\n",
    "# classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Fitting ANN to the training set\n",
    "# classifier.fit(train_x, train_y, batch_size = 68, epochs = 2000, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Predicting the Test set results\n",
    "# pred_train_y = classifier.predict(train_x)\n",
    "# pred_train_y = (pred_train_y > 0.5)\n",
    "# # pred_train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels_tmp = []  \n",
    "# for label in pred_train_y:  \n",
    "#     tmp = []  \n",
    "#     if label[0] == True:  \n",
    "#         tmp = [1,0]  \n",
    "#     else:   \n",
    "#         tmp = [0,1]  \n",
    "#     labels_tmp.append(tmp)  \n",
    "# new_result = np.array(labels_tmp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "# print('In-sample accuracy in Neural Network using Keras package (with all features):%s' % (accuracy_score(train_y, new_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Predicting the Test set results\n",
    "# pred_test_y = classifier.predict(test_x)\n",
    "# pred_test_y = (pred_test_y > 0.5)\n",
    "# # pred_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels_tmp = []  \n",
    "# for label in pred_test_y:  \n",
    "#     tmp = []  \n",
    "#     if label[0] == True:  \n",
    "#         tmp = [1,0]  \n",
    "#     else:   \n",
    "#         tmp = [0,1]  \n",
    "#     labels_tmp.append(tmp)  \n",
    "# new_result = np.array(labels_tmp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print('Out-of-sample accuracy in Neural Network using Keras package (with all features):%s' % (accuracy_score(test_y, new_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Is it enough of these two features to make predictions? \n",
    "# ## Neural Network with 2 features ('meanfun', 'IQR')\n",
    "# train_x = train_x[:,[5,12]]\n",
    "# test_x = test_x[:,[5,12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # It seems we can better result when using Keras deep learning library, \n",
    "# # so we will continue to use this package to do prediction.\n",
    "\n",
    "# #Initialisng the ANN\n",
    "# classifier = Sequential()\n",
    "\n",
    "# #Input Layer and First Hidden Layer\n",
    "# classifier.add(Dense(units = 512, activation = 'softmax', input_dim = train_x.shape[1]))\n",
    "\n",
    "# #Adding the Second hidden layer\n",
    "# classifier.add(Dense(units = 512, activation = 'softmax'))\n",
    "\n",
    "# # Adding the output layer\n",
    "# classifier.add(Dense(units = 2, activation = 'softmax'))\n",
    "\n",
    "# #Compiling the ANN\n",
    "# classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Fitting ANN to the training set\n",
    "# classifier.fit(train_x, train_y, batch_size = 68, epochs = 2000, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Predicting the Train set results\n",
    "# pred_y = classifier.predict(train_x)\n",
    "# pred_y = (pred_y > 0.5)\n",
    "# # pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels_tmp = []  \n",
    "# for label in pred_y:  \n",
    "#     tmp = []  \n",
    "#     if label[0] == True:  \n",
    "#         tmp = [1,0]  \n",
    "#     else:   \n",
    "#         tmp = [0,1]  \n",
    "#     labels_tmp.append(tmp)  \n",
    "# new_result = np.array(labels_tmp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print('In-sample accuracy in Neural Network using Keras package (with 2 features):%s' % (accuracy_score(train_y, new_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Predicting the Test set results\n",
    "# pred_y = classifier.predict(test_x)\n",
    "# pred_y = (pred_y > 0.5)\n",
    "# # pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels_tmp = []  \n",
    "# for label in pred_y:  \n",
    "#     tmp = []  \n",
    "#     if label[0] == True:  \n",
    "#         tmp = [1,0]  \n",
    "#     else:   \n",
    "#         tmp = [0,1]  \n",
    "#     labels_tmp.append(tmp)  \n",
    "# new_result = np.array(labels_tmp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print('Out-of-sample accuracy in Neural Network using Keras package (with 2 features):%s' % (accuracy_score(test_y, new_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "000\n",
    "In case with only 2 features(‘meanfun’, ‘IQR’) without feature scaling results are more precise. \n",
    "\n",
    "Neural Network with only two features gives 87.0% against 83.7% with all features. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
