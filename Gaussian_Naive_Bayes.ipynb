{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Naive Bayes to the Training set\n",
    "\n",
    "posterior = prior occurrances * liklihood / evidence\n",
    "The \"gaussian\" and \"naive\" come from two assumptions present in this likelihood:\n",
    "1. we assume each feature is uncorrelated from each other. That is, meannfreq is                  independent of sd or median etc.. This is obviously not true, and is a \"naive\"                assumption.\n",
    "2. we assume have that the value of the features (e.g. meannfreq of women) are normally          (gaussian) distributed. This means that p(meannfreq∣female) is calculated by inputing          the required parameters into the probability density function of the normal                    distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import pandas as pd \n",
    "import numpy as np  \n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%store -r train_x\n",
    "%store -r test_x\n",
    "%store -r train_y\n",
    "%store -r test_y\n",
    "\n",
    "%store -r train_x_two_features\n",
    "%store -r test_x_two_features\n",
    "%store -r train_y_two_features\n",
    "%store -r test_y_two_features\n",
    "\n",
    "%store -r transformed_train_x\n",
    "%store -r transformed_test_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_naiveBayes(train_x, test_x, train_y, test_y):\n",
    "    classifier = GaussianNB()\n",
    "    classifier.fit(train_x, train_y)\n",
    "    print('In-Sample Accuracy:{:.4f}%'.format(classifier.score(train_x, train_y)*100))\n",
    "    print('Out-of-Sample Accuracy:{:.4f}%'.format(classifier.score(test_x, test_y)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with all features(Built-in Algorithm):\n",
      "In-Sample Accuracy:89.5202%\n",
      "Out-of-Sample Accuracy:90.0253%\n"
     ]
    }
   ],
   "source": [
    "print(\"with all features(Built-in Algorithm):\")\n",
    "run_naiveBayes(train_x, test_x, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second Approach: We wrote our own method to implement  https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/\n",
    "def calculateProbability(x, mean, variance_y):\n",
    "    exp = math.exp(-(math.pow(x-mean, 2)/(2*math.pow(variance_y, 2))))\n",
    "    result = (1/(math.sqrt(2*math.pi)*variance_y))*exp\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculateClassProbabilities(data, inputVector):\n",
    "    probabilities = {}\n",
    "    for classValue, classdata in data.items():\n",
    "        probabilities[classValue] = 1\n",
    "        for i in range(len(classdata)):    \n",
    "            mean, stdev = classdata[i]\n",
    "            x = inputVector[i]\n",
    "            probabilities[classValue] *= calculateProbability(x, mean, stdev)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(summaries, inputVector):\n",
    "    probabilities = calculateClassProbabilities(summaries, inputVector)\n",
    "    bestLabel, bestProb = None, -1\n",
    "    for classValue, probability in probabilities.iteritems():\n",
    "        if bestLabel is None or probability > bestProb:\n",
    "            bestProb = probability\n",
    "            bestLabel = classValue\n",
    "    return bestLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPredictions(summaries, testSet):\n",
    "    predictions = []\n",
    "    for i in range(len(testSet)):\n",
    "        result = predict(summaries, testSet[i])\n",
    "        predictions.append(result)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAccuracy(testSet, predictions):\n",
    "    correct = 0\n",
    "    for x in range(len(testSet)):\n",
    "        if testSet[x][-1] == predictions[x]:\n",
    "            correct += 1\n",
    "    return (correct/float(len(testSet))) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separateByClass(dataset):\n",
    "    separated = {}\n",
    "    for i in range(len(dataset)):\n",
    "        vector = dataset[i]\n",
    "        if (vector[-1] not in separated):\n",
    "            separated[vector[-1]] = []\n",
    "        separated[vector[-1]].append(vector)\n",
    "    return separated\n",
    "\n",
    "def summarizeByClass(dataset):\n",
    "    separated = separateByClass(dataset)\n",
    "    summaries = {}\n",
    "    for classValue, instances in separated.iteritems():\n",
    "        summaries[classValue] = summarize(instances)\n",
    "    return summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # For both the classes 0 and 1, and for each of the 20 features, we have thus saved the mean and the standard in summary. \n",
    "# # For example, for the class 0 (female), and feature 1 (ie, sd), mean and the standard for this attribute are 0.060910268026077803 and 0.00034668419803097784\n",
    "\n",
    "# def getSummary(data,n_cls):\n",
    "#     data_means = data.groupby('label').mean()\n",
    "#     data_variance = data.groupby('label').var()\n",
    "    \n",
    "#     cnames = np.array(data.columns.tolist())[:-1] \n",
    "#     classes = np.unique(labels)\n",
    "#     classnames = np.array(['female','male'])\n",
    "#     summary = {}\n",
    "\n",
    "#     for cls in classes:\n",
    "#         #initializing the dictionary\n",
    "#         summary[cls] = defaultdict(list)\n",
    "#     for cls in classes:\n",
    "#         for j in range(0, n_cls):\n",
    "#             print(classnames[cls])\n",
    "#             summary[cls][j] += list(np.array([data_means[cnames[j]][data_means.index == classnames[cls]].values[0],\n",
    "#                                           data_variance[cnames[j]][data_variance.index == classnames[cls]].values[0]]))\n",
    "            \n",
    "#     print summary   \n",
    "#     return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summary = getSummary(train_df,train_x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Group the data by gender and calculate the means of each feature\n",
    "# data_means = train_x_df.groupby('label').mean()\n",
    "# # View the values\n",
    "# data_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Group the data by gender and calculate the variance of each feature\n",
    "# data_variance = train_x_df.groupby('label').var()\n",
    "# # View the values\n",
    "# data_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Create a function to calculate the probability density of each of the terms of the likelihood \n",
    "# # We use mean and standard deviation of input values (x) for each class to summarize the distribution\n",
    "# # Probabilities of new x values are calculated using the Gaussian Probability Density Function (PDF).\n",
    "\n",
    "# def p_x_given_y(x, mean_y, variance_y):\n",
    "#     # Input the arguments into a probability density function\n",
    "#     p = 1/(np.sqrt(2*np.pi*variance_y)) * np.exp((-(x-mean_y)**2)/(2*variance_y))\n",
    "#     return p\n",
    "\n",
    "# # We calculate the probability that the value of the random variable will be between x - 0.01 and x + 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Test\n",
    "# p_x_given_y(2, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def calculateClassProbabilities(summaries, inputVector):\n",
    "#     # p(x|y) (e.g. p(meannfreq∣female))\n",
    "#     probabilities = {}\n",
    "#     for classValue, classSummaries in summaries.iteritems():\n",
    "#         if (classValue == 0):\n",
    "#             probabilities[classValue] = P_female\n",
    "        \n",
    "#         else:\n",
    "#             probabilities[classValue] = P_male\n",
    "#         for i in range(len(classSummaries)):\n",
    "#             mean, var = classSummaries[i]\n",
    "#             x = inputVector[i]\n",
    "#             probabilities[classValue] *= p_x_given_y(x, mean, var)\n",
    "#     return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculateClassProbabilities(summary, test_x[10])\n",
    "# # PDF can be greater than 1, but it can't be greater than 1 for a large interval\n",
    "# # because of the non-negativity and integral constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Make a Prediction\n",
    "# import operator\n",
    "# def predict(summaries, inputVector):\n",
    "#     probabilities = calculateClassProbabilities(summaries, inputVector)\n",
    "#     #print probabilities\n",
    "#     probabilities = sorted(probabilities.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "#     return probabilities[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(test_x[10])\n",
    "# print('Our prediction for test_x[10] is %s'  % (predict(summary, test_x[10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# def GaussianNB_Classifier(test_x,test_y):\n",
    "#     results = []\n",
    "#     for i in range(len(test_x)):\n",
    "#         test_single = test_x[i]\n",
    "#         res = predict(summary, test_single)\n",
    "#         results.append(res)\n",
    "#     # We evaluate the accuracy of the model by calculating a ratio of the total correct predictions out of all predictions made (the classification accuracy)\n",
    "#     correct = 0\n",
    "#     wrong = 0\n",
    "#     for j in range(len(test_y)):\n",
    "#         if(results[j] == test_y[j]):\n",
    "#             correct += 1\n",
    "#         else:\n",
    "#             wrong += 1\n",
    " \n",
    "#     print('Correct rate is %s'  % (float(correct)/(correct+wrong)))\n",
    "#     print(classification_report(test_y,results))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(\"Use classification reports to evaluate how well Gaussian Naive Bayes performed with all features \\n\")\n",
    "# print(\"in-sample accuracy in Gaussian Naive Bayes:\")\n",
    "# GaussianNB_Classifier(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(\"out-of-sample accuracy in Gaussian Naive Bayes:\")\n",
    "# GaussianNB_Classifier(test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # We are wondering if there is a big difference in the results \n",
    "# # when used Sklearn package vs. our own algorithm\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# nb = GaussianNB()\n",
    "# nb.fit(train_x, train_y)\n",
    "# prediction = nb.predict(test_x)\n",
    "\n",
    "# print('NB result: ', accuracy_score(test_y, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Is it enough of these two features to make predictions? \n",
    "# ## Random Forest with 2 features ('meanfun', 'IQR')\n",
    "\n",
    "# voice_new_data = voice_data[['IQR','meanfun','label']]\n",
    "# train_new_x = train_x[:,[5,12]]\n",
    "# test_new_x = test_x[:,[5,12]]\n",
    "# summary = getSummary(voice_new_data, train_new_x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(\"Use classification reports to evaluate how well Gaussian Naive Bayes performed with only 2 features \\n\")\n",
    "# print(\"In-sample accuracy in Gaussian Naive Bayes:\")\n",
    "# GaussianNB_Classifier(train_new_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(\"Out-of-sample accuracy in Gaussian Naive Bayes:\")\n",
    "# GaussianNB_Classifier(test_new_x,test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
