{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iii)-5. Implementation of Decision Tree \n",
    "This file is the implementation of the Decision Tree. In this section, the group generated their own decision tree, and compare its accuracy with the accuracy of the decision tree generated by Sklearn pakage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii)-5-A Decision Tree Generated by Sklearn Package \n",
    "This section shows the decision tree generated by Sklearn package. The second unit shows the accuracy of the complete tree with all the features, and the third one shows the accuracy of the complete tree with only relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fei/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/fei/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "import graphviz\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-Sample Accuracy:100.00%\n",
      "Out-of-Sample Accuracy:78.26%\n",
      "\n",
      "In-Sample Confusion Table:\n",
      "              pred_Male  pred_Female\n",
      "true_Male          179            0\n",
      "true_Female          0          189\n",
      "Out-of-Sample Confusion Table:\n",
      "              pred_Male  pred_Female\n",
      "true_Male           38           13\n",
      "true_Female          7           34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Decision_Tree_All_Features.pdf'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "        Complete Decision Tree With All Features\n",
    "'''\n",
    "#--------------------Import and pre-processe data--------------------\n",
    "alleletronicsdata = pd.read_csv('voice.csv')\n",
    "# Split traing and testing set \n",
    "x_train, x_test, y_train, y_test = train_test_split(alleletronicsdata.iloc[:,0:-1],alleletronicsdata.iloc[:,-1], test_size=0.2,random_state=1)\n",
    "\n",
    "\n",
    "#--------------------Train.Fit Decision Tree.--------------------\n",
    "decisiontree = DecisionTreeClassifier()\n",
    "decisiontree.fit(x_train, y_train)\n",
    "\n",
    "#--------------------Test trained tree.--------------------\n",
    "print('In-Sample Accuracy:{:.2f}%'.format(decisiontree.score(x_train, y_train)*100))\n",
    "print('Out-of-Sample Accuracy:{:.2f}%\\n'.format(decisiontree.score(x_test, y_test)*100))\n",
    "\n",
    "#--------------------Confusion Table.--------------------\n",
    "# predict the testing data\n",
    "y_predict_train = decisiontree.predict(x_train)\n",
    "y_predict_test = decisiontree.predict(x_test)\n",
    "\n",
    "# in-sample\n",
    "confusionmtrx_train = pd.DataFrame(confusion_matrix(y_pred=y_predict_train, y_true=y_train), \n",
    "                                   index=['true_Male', 'true_Female'],columns=['pred_Male', 'pred_Female'])\n",
    "print(\"In-Sample Confusion Table:\\n\",confusionmtrx_train)\n",
    "\n",
    "#out-of-sample\n",
    "confusionmtrx_test = pd.DataFrame(confusion_matrix(y_pred=y_predict_test, y_true=y_test), index=['true_Male', 'true_Female'],\n",
    "                  columns=['pred_Male', 'pred_Female'])\n",
    "print(\"Out-of-Sample Confusion Table:\\n\",confusionmtrx_test)\n",
    "\n",
    "#--------------------Print trained tree--------------------\n",
    "#The complete tree is too big to be shown here. \n",
    "#Please refer to the pdf file(Decision_Tree_All_Features.pdf) in the folder.\n",
    "dot_data = tree.export_graphviz(decisiontree, out_file=None,  class_names=['male', 'female'], impurity=True, filled=True, rounded=True, special_characters=True) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"Decision_Tree_All_Features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-Sample Accuracy:100.00%\n",
      "Out-of-Sample Accuracy:83.70%\n",
      "\n",
      "In-Sample Confusion Table:\n",
      "              pred_Male  pred_Female\n",
      "true_Male          179            0\n",
      "true_Female          0          189\n",
      "Out-of-Sample Confusion Table:\n",
      "              pred_Male  pred_Female\n",
      "true_Male           39           12\n",
      "true_Female          3           38\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "        Complete Decision Tree With Only Relevant Features \n",
    "'''\n",
    "#--------------------Reduce features--------------------\n",
    "columns_to_use = [ 'IQR',  'meanfun','label']\n",
    "alleletronicsdata = alleletronicsdata.loc[:, columns_to_use].copy()\n",
    "x_train, x_test, y_train, y_test = train_test_split(alleletronicsdata.iloc[:,0:-1],alleletronicsdata.iloc[:,-1], test_size=0.2,random_state=1)\n",
    "\n",
    "#--------------------Train.Fit Decision Tree.--------------------\n",
    "decisiontree = DecisionTreeClassifier()\n",
    "decisiontree.fit(x_train, y_train)\n",
    "\n",
    "#--------------------Test trained tree.--------------------\n",
    "print('In-Sample Accuracy:{:.2f}%'.format(decisiontree.score(x_train, y_train)*100))\n",
    "print('Out-of-Sample Accuracy:{:.2f}%\\n'.format(decisiontree.score(x_test, y_test)*100))\n",
    "\n",
    "#--------------------Confusion Table.--------------------\n",
    "# predict the testing data\n",
    "y_predict_train = decisiontree.predict(x_train)\n",
    "y_predict_test = decisiontree.predict(x_test)\n",
    "\n",
    "# in-sample\n",
    "confusionmtrx_train = pd.DataFrame(confusion_matrix(y_pred=y_predict_train, y_true=y_train), \n",
    "                                   index=['true_Male', 'true_Female'],columns=['pred_Male', 'pred_Female'])\n",
    "print(\"In-Sample Confusion Table:\\n\",confusionmtrx_train)\n",
    "\n",
    "#out-of-sample\n",
    "confusionmtrx_test = pd.DataFrame(confusion_matrix(y_pred=y_predict_test, y_true=y_test), index=['true_Male', 'true_Female'],\n",
    "                  columns=['pred_Male', 'pred_Female'])\n",
    "print(\"Out-of-Sample Confusion Table:\\n\",confusionmtrx_test)\n",
    "#--------------------Print trained tree.--------------------\n",
    "#The complete tree is too big to be shown here. \n",
    "#Please refer to the pdf file(Decision_Tree_Two_Features.pdf) in the folder.\n",
    "dot_data = tree.export_graphviz(decisiontree, out_file=None,  class_names=['male', 'female'], impurity=True, filled=True, rounded=True, special_characters=True) \n",
    "graph = graphviz.Source(dot_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii)-5-B Decision Tree Generated Without Using Sklearn Package \n",
    "This section shows the decision tree written by the group. The first 9 cells are importation of packages, and definition of functions and class. The 10th is the training and testing of complete tree. The last code unit shows the pruning process of the tree, and the accuracy of pruned tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import random as R\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from subprocess import call\n",
    "import math \n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import subprocess\n",
    "import random as R\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the .csv data file \n",
    "\n",
    "#--------------------Fetch Data From File--------------------\n",
    "def GetData(File):  \n",
    "    if os.path.exists(File):  #if file exists \n",
    "        df = pd.read_csv(File, index_col=None)# read file\n",
    "        df['label'] = df['label'].map({'male': 0, 'female': 1})  #Chaneg gender label to binary code \n",
    "        # Drop unecessary columns, adjust sequence:\n",
    "        df = df.dropna(axis=0, how='any')   #Drop instances with \"NaN\" \n",
    "    else:\n",
    "        print(\"File not found.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a dataset with one independent attribute and a the dependent attribute, gender label, and a threshold, \n",
    "# this function returns the gini index of the sub sets splited based on the threshold. \n",
    "# Gini index is chosen over information entropy here, because log operation has a higher complexity. \n",
    "\n",
    "#------------------CalCulate GiniIndex------------------\n",
    "def GiniIndex(DataSet,Th):\n",
    "    gini=1.0       # initialize gini index to be 1\n",
    "    m,n=DataSet.shape  \n",
    "\n",
    "    #Counters       \n",
    "    Count_L_M=0.0  #Counters for Male with attribute lower that threshold\n",
    "    Count_L_F=0.0  #Counters for Female with attribute lower that threshold\n",
    "    Count_H_M=0.0  #Counters for Male with attribute higher that threshold\n",
    "    Count_H_F=0.0  #Counters for Female with attribute higher that threshold\n",
    "\n",
    "    #A: The independent attribute; B: Gender label \n",
    "    A=DataSet['A']\n",
    "    B=DataSet['B']\n",
    "    \n",
    "    # Count instances fall into each class(M/F) in each side (H?L) \n",
    "    for i in range(0,m):\n",
    "        if A[i]<Th:\n",
    "            if B[i]==0:\n",
    "                Count_L_M+=1\n",
    "            else:\n",
    "                Count_L_F+=1\n",
    "        else:\n",
    "            if B[i]==0:\n",
    "                Count_H_M+=1\n",
    "            else:\n",
    "                Count_H_F+=1\n",
    "    \n",
    "    TotalLow=Count_L_M+Count_L_F\n",
    "    TotalHigh=Count_H_M+Count_H_F\n",
    "    \n",
    "    \n",
    "    P_L_M=Count_L_M/TotalLow\n",
    "    P_L_F=Count_L_F/TotalLow\n",
    "    P_H_M=Count_H_M/TotalHigh\n",
    "    P_H_F=Count_H_F/TotalHigh\n",
    "\n",
    "    gini=P_L_M*(1-P_L_M)+P_L_F*(1-P_L_F)+P_H_M*(1-P_H_M)+P_H_F*(1-P_H_F)\n",
    "\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a dataset with one independent attribute and a the dependent attribute, \n",
    "# this function returns the best splitting point of this attribute, and the gini \n",
    "# index of this split. GiniIndex() function is called in this function.\n",
    "#------------------Find the Best Point to Split------------------\n",
    "def FindSplitPoint(DataSet):   \n",
    "    Th=0.0\n",
    "    gini=1\n",
    "    \n",
    "    Min=DataSet['A'].min()\n",
    "    Max=DataSet['A'].max()\n",
    "    step=(Max-Min)/100.0  \n",
    "    t=Min+step\n",
    "    \n",
    "    while t<Max:\n",
    "        g=GiniIndex(DataSet,t)\n",
    "        if g<gini:\n",
    "            gini=g\n",
    "            Th=t\n",
    "        else:\n",
    "            pass\n",
    "        t+=step    \n",
    "    return Th,gini\n",
    "\n",
    "#---------Function Test-------\n",
    "#print(FindSplitPoint(DataSet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Given the whole dataset, the best attribute to split this data set and the splitting threshold.\n",
    "\n",
    "#------------------Split Data According to chosen Attribute------------------\n",
    "def SplitData(DataSet,Attribute,Th):\n",
    "    LowSide=DataSet[DataSet[Attribute]<Th]\n",
    "    HighSide=DataSet[DataSet[Attribute]>=Th]\n",
    "    return LowSide,HighSide\n",
    "\n",
    "#--------Function Test-------------\n",
    "#L,H=SplitData(df,'sd',0.096502470739196294)\n",
    "#print(L)\n",
    "#print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##------------------Define Class node------------------\n",
    "class node:             #Nodes in the tree\n",
    "    def __init__(self,Data):\n",
    "        self.name=None\n",
    "        self.left=None\n",
    "        self.right=None\n",
    "        self.Threshold = None\n",
    "        self.prev=None\n",
    "        self.data=Data\n",
    "        \n",
    "    def prev(self):\n",
    "        return self.prev\n",
    "                   \n",
    "    def AddLeft(self,NewData):    # add left child to node\n",
    "        node1=node(NewData)\n",
    "        self.left=node1\n",
    "        node1.prev=self\n",
    "        return node1\n",
    "    \n",
    "    def AddRight(self,NewData):   # add right child to node\n",
    "        node1=node(NewData)\n",
    "        self.right=node1\n",
    "        node1.prev=self\n",
    "        return node1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Given the root node of the decision tree(in which the whole data set is included), this function will \n",
    "#grow the decision tree based on the data set. FindSplitPoint() is called in this function. \n",
    "#In addition, this is an iteration function.\n",
    "\n",
    "#------------------ Grow Tree Given Root Node ------------------\n",
    "def Grow(node):\n",
    "    m,n=node.data.shape\n",
    "       \n",
    "    # The tree should stop growing if the labe is all in one gender or there is no independent attribute in data\n",
    "    Purity=node.data['label'].sum()     #label: MAle=0;Female=1\n",
    "    Flag=(n>1) and (Purity>0) and (Purity<m) #Flag=True: grow tree; else： stop \n",
    "    #---------Stop Growing----------\n",
    "    if Flag==False:      #Generate Leaf node with no children\n",
    "        if (Purity<m/2):   # All or most of the instances are labeled \"Male\"\n",
    "            node.name='Male'\n",
    "        else:              # All or most of the instances are labeled \"Female\"\n",
    "            node.name='Female'\n",
    "    \n",
    "    #---------Grow Tree-------------\n",
    "    else:  \n",
    "        #Find Best Attribute to split data\n",
    "        gini=2\n",
    "        Th=0\n",
    "        Attribute=''\n",
    "        \n",
    "        for i in range(0,n-1):\n",
    "            DataSet=pd.DataFrame({'A':node.data[node.data.columns[i]],'B':node.data['label']})\n",
    "            t,g=FindSplitPoint(DataSet)\n",
    "            if g<gini:     # Record this attribute if it has better ginin than the previous ones \n",
    "                gini=g\n",
    "                Th=t\n",
    "                Attribute=node.data.columns[i]\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        # Split data and DROP PROCESSED ATTRIBUTE\n",
    "        LeftData,RightData=SplitData(node.data,Attribute,Th)\n",
    "        #DROP PROCESSED ATTRIBUTE\n",
    "        LeftData.drop([Attribute],axis=1,inplace=True)\n",
    "        RightData.drop([Attribute],axis=1,inplace=True)\n",
    "        #Adjust index after the split\n",
    "        index_L=np.linspace(0, len(LeftData.index)-1,num=len(LeftData.index), dtype=int)\n",
    "        index_R=np.linspace(0, len(RightData.index)-1,num=len(RightData.index), dtype=int)\n",
    "        LeftData.index=index_L\n",
    "        RightData.index=index_R\n",
    "        \n",
    "        # Fix current node\n",
    "        node.name=Attribute\n",
    "        node.Threshold=Th\n",
    "        \n",
    "        # Add children nodes\n",
    "        LeftNode=node.AddLeft(LeftData)\n",
    "        RightNode=node.AddRight(RightData)\n",
    "      \n",
    "        # Iteration: Grow children node  \n",
    "        Grow(LeftNode)\n",
    "        Grow(RightNode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Print put the stricture of grown tree. For test only\n",
    "\n",
    "#------------------Print Tree------------------\n",
    "def PrintTree(Root,level):\n",
    "    TAB=\"      \"*level\n",
    "    print(TAB,Root.name,\"\\n\")\n",
    "    if Root.left!=None:\n",
    "          PrintTree(Root.left,level=level+1)\n",
    "    if Root.right!=None:\n",
    "          PrintTree(Root.right,level=level+1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given the root of the grown decision tree and the test set, this function calculate the accuracy of the prediction, \n",
    "# and print out a confusion table \n",
    "\n",
    "#--------------------After-Train Prediction--------------------\n",
    "def Predict(root,test):\n",
    "    p,q=test.shape\n",
    "    Counter_MM=0 #Reconginze Male as Male\n",
    "    Counter_MF=0 #Reconginze Male as Female\n",
    "    Counter_FM=0 #Reconginze Female as Male\n",
    "    Counter_FF=0 #Reconginze Female as Female\n",
    "    \n",
    "    for i in range(0,p):  # for each new insatance, go over the whole tree t9 opredict its class\n",
    "        \n",
    "        #Go over Tree. Start with root node, end with leaf node\n",
    "        Current=root\n",
    "        while(Current.left!=None):\n",
    "            if test[Current.name][i]<Current.Threshold:\n",
    "                Current=Current.left\n",
    "            else:\n",
    "                Current=Current.right\n",
    "        \n",
    "        gender=Current.name\n",
    "        if gender=='Male':\n",
    "            if test['label'][i]==0:\n",
    "                Counter_MM+=1\n",
    "            else:\n",
    "                Counter_FM+=1\n",
    "        elif gender=='Female':\n",
    "            if test['label'][i]==1:\n",
    "                Counter_FF+=1\n",
    "            else:\n",
    "                Counter_MF+=1\n",
    "   \n",
    "    AccuracyScore=(Counter_MM+Counter_FF)/p\n",
    "\n",
    "    print(AccuracyScore)\n",
    "    print(\"Recogninze Male as Male:\",Counter_MM)\n",
    "    print(\"Recogninze Male as Female:\",Counter_MF)\n",
    "    print(\"Recogninze Female as Male:\",Counter_FM)\n",
    "    print(\"Recogninze Female as Female:\",Counter_FF)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fei/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/fei/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With all 20 attributes \n",
      "Out of sample accuracy:\n",
      "0.8043478260869565\n",
      "Recogninze Male as Male: 37\n",
      "Recogninze Male as Female: 4\n",
      "Recogninze Female as Male: 14\n",
      "Recogninze Female as Female: 37\n",
      "\n",
      "In sample accuracy:\n",
      "0.9266304347826086\n",
      "Recogninze Male as Male: 180\n",
      "Recogninze Male as Female: 9\n",
      "Recogninze Female as Male: 18\n",
      "Recogninze Female as Female: 161\n",
      "\n",
      "\n",
      "With relevant attributes \n",
      "Out of sample accuracy:\n",
      "0.8478260869565217\n",
      "Recogninze Male as Male: 38\n",
      "Recogninze Male as Female: 3\n",
      "Recogninze Female as Male: 11\n",
      "Recogninze Female as Female: 40\n",
      "\n",
      "In sample accuracy:\n",
      "0.8967391304347826\n",
      "Recogninze Male as Male: 173\n",
      "Recogninze Male as Female: 16\n",
      "Recogninze Female as Male: 22\n",
      "Recogninze Female as Female: 157\n"
     ]
    }
   ],
   "source": [
    "#---------------------Main Function ---------------------\n",
    "# Import data from file\n",
    "df=GetData(\"voice.csv\") \n",
    "\n",
    "#Split the whole set into Train set and Test set\n",
    "Train,Test =train_test_split(df,test_size=460-368, train_size=368, random_state=1)\n",
    "#Adjust the index of Test and Train after the split \n",
    "index_train=np.linspace(0, len(Train.index)-1,num=len(Train.index), dtype=int)\n",
    "index_test=np.linspace(0, len(Test.index)-1,num=len(Test.index), dtype=int)\n",
    "Train.index=index_train\n",
    "Test.index=index_test\n",
    "\n",
    "#Creat root node for the tree, and then grow it \n",
    "root_Full=node(Train)\n",
    "Grow(root_Full)  \n",
    "\n",
    "#Test the grown treen with Test data\n",
    "print(\"With all 20 attributes \")\n",
    "print(\"Out of sample accuracy:\")\n",
    "Predict(root_Full,Test)\n",
    "print(\"\\nIn sample accuracy:\")\n",
    "Predict(root_Full,Train)\n",
    "\n",
    "#-----------------Drop Irrelevant Attribute--------------------\n",
    "df.drop(['meanfreq', 'sd', 'median', 'Q25', 'Q75', 'skew', 'kurt',\n",
    "       'sp.ent', 'sfm', 'mode', 'centroid', 'minfun', 'maxfun',\n",
    "       'meandom', 'mindom', 'maxdom', 'dfrange', 'modindx'],\n",
    "        axis=1,inplace=True)\n",
    "#Split the whole set into Train set and Test set\n",
    "Train,Test =train_test_split(df,test_size=460-368, train_size=368, random_state=1)\n",
    "#Adjust the index of Test and Train after the split \n",
    "index_train=np.linspace(0, len(Train.index)-1,num=len(Train.index), dtype=int)\n",
    "index_test=np.linspace(0, len(Test.index)-1,num=len(Test.index), dtype=int)\n",
    "Train.index=index_train\n",
    "Test.index=index_test\n",
    "\n",
    "#Creat root node for the tree, and then grow it \n",
    "root_Two=node(Train)\n",
    "Grow(root_Two)    \n",
    "\n",
    "#Test the grown treen with Test data\n",
    "print(\"\\n\\nWith relevant attributes \")\n",
    "print(\"Out of sample accuracy:\")\n",
    "Predict(root_Two,Test)\n",
    "print(\"\\nIn sample accuracy:\")\n",
    "Predict(root_Two,Train)\n",
    "# Data Set For Function Test    \n",
    "#DataSet=pd.DataFrame({'A':df['sd'],'B':df['label']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sfm pruned\n",
      "modindx pruned\n",
      "dfrange pruned\n",
      "maxdom pruned\n",
      "mindom pruned\n",
      "maxfun pruned\n",
      "minfun pruned\n",
      "centroid pruned\n",
      "mode pruned\n",
      "sp.ent pruned\n",
      "skew pruned\n",
      "IQR pruned\n",
      "Q75 pruned\n",
      "Q25 pruned\n",
      "sd pruned\n",
      "meanfreq pruned\n",
      "maxdom pruned\n",
      "dfrange pruned\n",
      "mindom pruned\n",
      "meandom pruned\n",
      "Q25 pruned\n",
      "centroid pruned\n",
      "mode pruned\n",
      "sfm pruned\n",
      "sp.ent pruned\n",
      "kurt pruned\n",
      "skew pruned\n",
      "IQR pruned\n",
      "median pruned\n",
      "sd pruned\n",
      "meanfreq pruned\n",
      " meanfun \n",
      "\n",
      "       meandom \n",
      "\n",
      "             Male \n",
      "\n",
      "             Female \n",
      "\n",
      "       maxfun \n",
      "\n",
      "             Male \n",
      "\n",
      "             Q75 \n",
      "\n",
      "                   modindx \n",
      "\n",
      "                         Female \n",
      "\n",
      "                         Male \n",
      "\n",
      "                   Male \n",
      "\n",
      "Pruned Tree:\n",
      "Out of sample accuracy:\n",
      "0.8152173913043478\n",
      "Recogninze Male as Male: 38\n",
      "Recogninze Male as Female: 3\n",
      "Recogninze Female as Male: 14\n",
      "Recogninze Female as Female: 37\n",
      "\n",
      "In sample accuracy:\n",
      "0.9157608695652174\n",
      "Recogninze Male as Male: 179\n",
      "Recogninze Male as Female: 10\n",
      "Recogninze Female as Male: 21\n",
      "Recogninze Female as Female: 158\n"
     ]
    }
   ],
   "source": [
    "#------------------Chi Square Pruning-------------\n",
    "'''\n",
    "    The group choose the threshold of Q to be 0.15, after observing a the few Q values of most of the nodes.\n",
    "    P_chance, i.e., the chance that  the attribute is irrelevant, is set to be 70%. Any attribute with P_chance lower\n",
    "    than this value (a Q < 0.15), will be pruned. The pruning process uses post-order traversal. The function also \n",
    "    printd out the attributes that get pruned.\n",
    "'''\n",
    "\n",
    "def ChiSquarePrune(root):\n",
    "\n",
    "    def recurse(node):\n",
    "        if node.left.left != None:\n",
    "            recurse(node.left)\n",
    "        if node.right.left!=None:\n",
    "            recurse(node.right)\n",
    "        \n",
    "        S,ftr=node.data.shape         # S:total number of instances; ftr:total number of attributes\n",
    "        S=float(S)\n",
    "        p=float(node.data['label'].sum())    # p: total number of female\n",
    "        n=S-p                         # Total number of male \n",
    "    \n",
    "        Data_Low,Data_High=SplitData(node.data,node.name,node.Threshold)\n",
    "        S_L=float(len(Data_Low))\n",
    "        S_H=float(len(Data_High))\n",
    "        p_L=float(Data_Low['label'].sum())\n",
    "        p_H=float(Data_High['label'].sum())\n",
    "        n_L=S_L-p_L\n",
    "        n_H=S_H-p_H\n",
    "    \n",
    "        p_L_hat=p/S*S_L\n",
    "        n_L_hat=n/S*S_L\n",
    "        p_H_hat=p/S*S_H\n",
    "        n_H_hat=n/S*S_H\n",
    "    \n",
    "        Q=pow((p_L-p_L_hat),2)/p_L_hat+pow((n_L-n_L_hat),2)/n_L_hat+pow((p_H-p_H_hat),2)/p_H_hat+pow((n_H-n_H_hat),2)/n_H_hat\n",
    "        if Q<0.5:\n",
    "            print(node.name,\"pruned\")\n",
    "            if p>S/2:\n",
    "                node.name='Female'\n",
    "            else:\n",
    "                node.name='Male'\n",
    "                \n",
    "            node.left=None\n",
    "            node.right=None\n",
    "            node.Th=None\n",
    "        \n",
    "        return\n",
    "    \n",
    "    recurse(root)\n",
    "\n",
    "    \n",
    "#-------------Prune the Complete Tree-------------\n",
    "root_pruned=root_Full\n",
    "ChiSquarePrune(root_pruned) \n",
    "PrintTree(root_pruned,0) #print pruned tree\n",
    "\n",
    "# Retest in-saple and out-of-sample accuracy\n",
    "df=GetData(\"voice.csv\") \n",
    "#Split the whole set into Train set and Test set\n",
    "Train,Test =train_test_split(df,test_size=460-368, train_size=368, random_state=1)\n",
    "#Adjust the index of Test and Train after the split \n",
    "index_train=np.linspace(0, len(Train.index)-1,num=len(Train.index), dtype=int)\n",
    "index_test=np.linspace(0, len(Test.index)-1,num=len(Test.index), dtype=int)\n",
    "Train.index=index_train\n",
    "Test.index=index_test\n",
    "#Print Result\n",
    "print(\"Pruned Tree:\")\n",
    "print(\"Out of sample accuracy:\")\n",
    "Predict(root_pruned,Test)\n",
    "print(\"\\nIn sample accuracy:\")\n",
    "Predict(root_pruned,Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## iv)-5. Discussion of Dicesion Tree \n",
    "Here are tables taht compare results:\n",
    "Accuracy of complete tree: \n",
    "\n",
    " |Out-Sample-20 Features|         \n",
    " |----------------------|\n",
    " | Sklearn | No Sklearn |\n",
    " | 78.26%  |   80.43%   |\n",
    " \n",
    " | In-Sample-20 Features|         \n",
    " |----------------------|\n",
    " | Sklearn | No Sklearn |\n",
    " | 100.0%  |   92.66%   |\n",
    " \n",
    "The in-sample accuracy is significantly higher than the out-of sample accuracy, which means there is overfitting in the model. The group tried to solve this problem by reducing irrelevant features, and pruning the complete tree.\n",
    "\n",
    " |Out-Sample-2 Features |         \n",
    " |----------------------|\n",
    " | Sklearn | No Sklearn |\n",
    " | 83.70%  |   84.78%   |\n",
    " \n",
    " | In-Sample-2 Features |         \n",
    " |----------------------|\n",
    " | Sklearn | No Sklearn |\n",
    " | 100.0%  |   89.67%   |\n",
    " \n",
    "Deleting irrelevant features improved the out-of-sample accuracy, but the in sample accuracy decreased. This is reasonable, as overfitting mainly causes by the model's attemption of fitting the inaccurate samples in the training set. The decrease in in-sample accuracy acctualy means that these inaccurate samples are excluded during training. The group also tried to prune the tree. Chi square test pruning is chosen here, simply because the group does not have sufficint data. \n",
    " \n",
    " Accuracy of pruned tree:\n",
    " \n",
    " | In-Sample | Out-Sample |\n",
    " |-----------|------------|\n",
    " |   81.52%  |   91.58%   |\n",
    "\n",
    "Both methods sucessfuly reduced overfitting, and raised the out-of-sample accuracy. Deleting all irrelevant features are more suceesful in improving this performance, but its shot coming is also very obvious. The relevance of each feature is evaluated by human after visualized the data, but the visulaization might hide some non-linear, and more complex relationship between the attributes and the classifycation. Pruning, on the other hand, is less agressive in deleting irrelevant features. In a simple task like this, in which most of the relationship are linear, directly deleting irrelevant features works better. \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
